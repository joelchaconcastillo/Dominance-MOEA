In this section the experimental validation is carried out, showing that controlling the diversity in the variable space is a way to improve further some of the results obtained by state-of-art-MOEAs.
%
At the beginning, several technical specifications taken into account in our comparison outline are explained.
%
Therafter, to have a broad perception of the \VSDMOEA{} some scenaries are analyzed.
%
Between them an analyzes driven to probe the scalability in the decision variable space is taken into account.
%
This analyzes is narrowed through some specific and well known problems.
%
In the same line, with the intention to have a better understanding of the critial parameter that induces the initial amount of diversity is taken into account.
%
This is carried out through several settings and with all the test-problems.
%
Finally, since that the mechanism imposed in our proposal, which avoid the premature convergence, highly depends on the elapsed time, we reported the benefits of our proposal considering both short, and long term executions.
%
The latter experiments shows that the \VSDMOEA{} has a decent performance in short-term executions too.
%

% which belong to jMetalcpp~\cite{Joel:JMETAL} framework.
%
To validate the proposed MOEA in this work are considered some of the most popular benchmark in the multi-objective field.
%
Particularly, the WFG~\cite{Joel:WFG}, DTLZ~\cite{Joel:DTLZ}, and UF~\cite{Joel:CEC2009} test problems have been used for our purpose. 
%
%Through the literature several crossover operators have been proposed in MOEAs~\cite{Joel:ParentMeanCentricSelfAdaptation},  
%a popular operator is the Simulated Binary Crossover (SBX).4\cite{Joel:SBX1994}%, Joel:TAXONOMY_CROSSOVER, Joel:Kalyanmoy}
Additionally, our experimental validation includes the \VSDMOEA{}, as well as three well-known state-of-the-art algorithms.
%
Given that all of them are stochastic algorithms, each execution was repeated $35$ times with different seeds.
%
The common configuration in all of them was the following: the stopping criterion was set to $250,000$ generations, the population size was fixed to $100$, the WFG test problems were configured with two and three objectives, which are setted to $24$ parameters, where $20$ of them are distance parameters, and $4$ are position parameters.
%
Additinally, in the DTLZ test instances, the number of decision variables is setted to $n=M+r-1$, where $r=\{5, 10, 20\}$ for DTLZ1, DTLZ2 to DTLZ6 and DTLZ7 respectively, as is suggested by the authors \cite{Joel:DTLZ}.  
% 
The UF benchmark is composed of ten test instances, which is categorized in two groups that are based in the number of objetive functions to solve, thus the first seven consists of two objectives, and the reaming of three, where the number of decision variables taken into account in each one is $n=30$.
%
In general, the crossover and mutation operators are the Simulated Binary Crossover (SBX), and polynomial~\cite{Joel:SBX1994, Joel:Mutation}, which probabilities are setted to $0.9$ and $1/n$ respectively.
%
Also, the crossover and mutation distribution indexes were assigned to $20$ and $50$ respectively.
%
The extra-parameterization of each algorithm is showed in the table~\ref{tab:Parametrization}.
%
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[t]
\centering
\caption{Parameterization of each MOEA considered}
\label{tab:Parametrization}
\begin{tabular}{|c|c|}
\hline
\textbf{Algorithm} & \textbf{Configuration} \\ \hline
\multirow{3}{*}{\textbf{MOEA/D}} &Max. updates by sub-problem ($\eta_r$) = 2, \\
 & tour selection = 10,   neighbour size = 10, \\
 & period utility updating = 30 generations \\ 
 & probability local selection ($\delta$) = 0.9,\\ \hline
\textbf{VSD-MOEA} & $D_I=\sqrt{n}*0.25$ \\ \hline
\textbf{R2-EMOA} & $\rho=1$, offspring by iteration = $1$ \\ \hline
\end{tabular}
\end{table}


Despite the fact that the \MOEAD{}, and \RMOEA{} can be employed with the same utility function --in this case the Tchebycheff function-- each one of them is designed through a notorious dissimilar paradigm.
%
Thus, the weight vectors taken into consideration for each one of them were different.%, i.e. the confias it was originally proposed by the authors.
%
The main reason of this, is that the \RMOEA{} can be configured with a different population size than the number of weight vectors without been significantly affected on its performance.
%
Particularly, the \RMOEA{} employes $501$ and $496$ weight vectors for two and three objectives respectively.
%
Contrarily, in the \MOEAD{} each weight vector is identified as a subproblem, therefore the population should correpond to the number of weight vectors.
%
Also, the weight vectors used in the \MOEAD{} should be uniformly scattered on the unit-simplex, however it can be a complication since that the number of vectors requiered for this task increases non-linearly according the number of objectives.
%
Therefore, in this version is appled the method proposed in \cite{Joel:MOEAD_Uniform_Design, Joel:Kuhn_Munkres} where the uniform design (UD) \cite{Joel:Uniform_Design} and good lattice point (GLP) are combined.
%
In this way, the number of weight vectors that is required by this \MOEA{} is not affected by the number of objectives.


Mainly, the experimental analyzes is carried out considering the hypervolume indicator (\HV{}).
%
The \HV{} metric measures the size of the objective space dominated by the approximated solutions given a reference point, so the solutions dominated by the reference point are not considered.
%
Particularly, the reference point is chosen to be a vector which values are sightly larger (ten percent) than the nadir point as is suggested in \cite{ishibuchi2017reference}.
%
Similarly that in \cite{li2015evolutionary}, and to have a fair comparison the normalized \HV{} is taken into account.
%
Specifically, the \HV{} reported is obtained as the division between the \HV{} reached by a set of solutions and the \HV{} of the optimal Pareto Front.
%
In this way, the more approximate to unity this metric is, the more a \MOEA{} reaches to the Pareto Front.
%



%%\begin{table}[t]
%%\centering
%%\caption{References points for the HV indicator}
%%\label{tab:ReferencePoints}
%%\begin{tabular}{cc}
%%\hline
%%\textbf{Instances} & \textbf{Reference Point} \\ \hline
%%WFG1-WFG9 & $[2.1, ...,2m+0.1]$ \\
%%DTLZ 1, 2, 4 & $[1.1, ..., 1.1]$ \\
%%DTLZ 3, 5, 6 & $[3, ..., 3]$ \\
%%DTLZ7 & $[1.1, ..., 1.1, 2m]$ \\
%%UF 1-10 & $[2, ..., 2]$ \\ \hline
%%\end{tabular}
%%\end{table}
%
In order to statistically compare the \HV{} results, a similar guideline than the proposed in~\cite{Joel:StatisticalTest} was used. 
%
First a Shapiro-Wilk test was performed to check whatever or not the values of the results followed a Gaussian distribution. 
%
If, so, the Levene test was used to check for the homogeneity of the variances. 
%
If samples had equal variance, an ANOVA test was done; if not, a Welch test was performed. 
%
For non-Gaussian distributions, the nonparametric Kruskal-Wallis test was used to test whether samples are drawn from the same distribution. 
%
An algorithm $X$ is said to win algorithm $Y$ when the differences between them are statistically significant, if the mean and median obtained by $X$ are higher than the mean and median achieved by $Y$.
%

In the tables \ref{tab:StatisticsHV_2obj}, \ref{tab:StatisticsHV_3obj} are showed the normalized hypervolume with two an three objectives respectively.
%
From this empirical results, it is clear that the \VSDMOEA{} obteained the highest general mean \HV{} with both two and three objectives.
%
Even more, the standard deviation is the lowest in almost all the problems, thus this \MOEA{} shows to be stable, meaning that reach similar results with differents seeds.
%
The best general mean (last row) considering two objectives is achieved by the \VSDMOEA{} with $0.955$.
%
Also, the second general mean is obtained by the \NSGAII{} with $0.886$.
%
Its important to remark that the general mean can be unsteady, since that if one problem is far a way from the Pareto front then this measure could be highly degraded. 
%
However, all the state-of-the-art-\MOEAS{} achieved a general mean of $0.88$ considering two objectives, whilst the \VSDMOEA{} obtained $0.95$.
%
Considering three objectives the performance of the \NSGAII{} is highly affected, this might occurs since that density estimator employed in the \NSGAII{} highly depends on the dominance-relation, thus in some problems (e.g. multi-frontal problems UF10) the solutions do not converge adequatly to the Pareto front.
%
In the same line, the \VSDMOEA{} achieved the best \HV{} values in almost all the problems, in fact such values that are higher than $0.9$ are close enough to the Pareto Front.
%
The second best \MOEA{} based in the general mean is the \RMOEA{} with $0.855$, despite that it adopts the same utility function that the \MOEAD{}, the latter is lightly lower with $0.835$, this might occurs given that the \MOEAD{} has several paremeters to be tunned, and the selected configuration could be insuficient to long-term executions.
%


In the tables \ref{tab:Tests_HV_2obj} and \ref{tab:Tests_HV_3obj} are showed the statistical tests with two an three objectives respectively.
%
In the column tagged with ``Diff'' is computed the difference between the mean of each algorithm and the best mean achieved.
%
Taking into account two objectives the \VSDMOEA{} achieved $52$ wins, in comparison the \RMOEA{} had $34$ wins.
%
In spite that the \NSGAII{} achieved a better general mean it won less times that the remaining \MOEAS{}.
%
Meaning that the \NSGAII{} obtains values close enough than the best \MOEA{}, this can be viewed in the last row where is computed the total sum of all the problems.
%
Generally speaking, the best results are obtained by the \VSDMOEA{}, since that the total sum of the ``Diff'' column is $0.061$, thus when this algorithm does not achieved the best results, it obtained near solutions to the best results.
%
Futhermore, the algorithms \RMOEA{} and \MOEAD{} achieved a similar total ``Diff'' values.
%
Particularly, the worst ``Diff'' value achieved by the \VSDMOEA{} is in the problem WFG6.
%
This problem is unimodal and non-separable, and this occurs since that the initial factor distance is very high, in fact through other experimental analyzes this instance was correctly solved with $D_I=0.1$ which mean achieved in this problem was of $0.917$ and $0.868$ for two and three objectievs respectively.
%
In addition, inspecting three objectives (table \ref{tab:Tests_HV_3obj}) it can be seen similar results, notably the \VSDMOEA{} improves its performance respectively to the remaining \MOEAS{}.
%
Particularly, the most complicated problems are better solved by the \VSDMOEA{} as are UF3, UF4, UF5, and UF6 in two objectives, and UF9, UF10 in three objectives.
%
The UF5 is considered as one of the most difficult problems since that the optimal Pareto front is conformed by $21$ points, also it has several suboptimal regions where the solutions can be stuck. 
%
Neverthless, the \MOEAS{} that considers weight vectors have more chances to get stuck, since that it is a highly disconected front.
%
It is showed, since that the \NSGAII{} has a lower ``Diff'' value ($0.048$) than the \MOEAD{} and \RMOEA{} ($0.205$ and $0.122$).
%
Diversely, the UF10 is a multi-frontal problem, this means that there exists different optimal non-dominated fronts that correspond to different locally optimal values \cite{huband2006review}, this characteristic increases the difficultness of the problem as the number of objectives increases.
%
Indeed, the latter problem has notably converged better in \VSDMOEA{} than the reamining \MOEAS{}.
%

%

%The column \textit{Diff} indicate how far is the best HV mean from each algorithm, this field is computed as the difference between the mean of the each algorithm and the best mean.
%
%Consequently, the best algorithm in each instance has assigned a zero.
%

%It is important to highlight that the DTLZ test suites have the next weaknesses \cite{Joel:CEC2009}.
%The global optimum lies in the center or bounds, all are separable and the global optimum has the same parameter values for different dimensions.
%
%Particularly the DTLZ5 and DTLZ6 are easy for the GDE3, since the global optimal are located in the low bound, thus if the differential operators locate a solution outside of bounds, the repair procedure could move the point among the optimal.

%
%Additionally, the GDE3 get worse according increases the number of objectives, even more the \textit{Diff} results are in average high, except for the DTLZ5 and DTLZ6 where the optimal set are located along the bounds.
%
%Despite the fact that the VSD-MOEA does not have a repair procedure because it implements genetic operators, the results are fairly stable.
%

%Also improves the hypervolume with three objectives, hence can be considered a robust algorithm, in fact considering more objectives it provides better results than the dominance-based algorithms\footnote{Experimentally with ten objectives the algorithm is best than dominance-based MOEAs.}.
%
%A remarkable characteristic that can be appreciated in two and three objectives is that our  proposal provide the best results in the most difficult problems as are dependence, multi-modal and deceptiveness.

%
%On average the VSD-EMOA has lower \textit{Diff} value than the rest of algorithms, therefore considering all best mean of each instance, the VSD-EMOA is not too far from the best means.
%
%Even more, the min and max averages are better than the min and max of the state-of-art algorithms.
%

%The effective test showed in the tables \ref{tab:Effective_Test_2obj}, \ref{tab:Effective_Test_3obj} are conformed by two an three objectives respectively,  these metrics qualify the superiority of each algorithm with the rest through pair comparisons.
%
%Thus, an algorithm \textbf{A} is compared with algorithm \textbf{B}, if \textbf{A} wins, the difference with \textbf{B} is accumulated in wins $\uparrow$ of the algorithm \textbf{A}, the same process is for the algorithm \textbf{B} but it is accumulated in the lost field $\downarrow$.
%
%The column \textit{Score} is conformed by the difference between the win and lost values, therefore a high positive score indicate the superiority of the algorithm.
%

%In addition, our proposal has low negative scores in the DTLZ6 and WFG6 with two objectives, it might occurs because these instances are not a problem to long term executions, therefore they are totally defined by the crowding procedure.
%
%However, the negative scores in VSD-MOEA are not highly significant.
%
%Additionally, considering three objectives, the VSD-MOEA provides the best and positive scores.
%
%In general our proposal has the best scores in difficult instances as are the UF and some WFG problems.

\subsection{Decision Variable Scalability Experiments}

The scalability of each MOEA is also evaluated respect with the number of decision variables \cite{Joel:ScalabilityStudy}. 
%
The figures \ref{fig:Scalability_Study_HV_1}, \ref{fig:Scalability_Study_HV_2} show the hypervolume performance of 30, 100, 250 and 500 variables respectively.
%
Particularly,  the scalability study was realized in DTLZ4, UF5 with two objectives and DTLZ4, UF10 with three objectives.
%
In some instances the GDE3 degrades relatively fast according increases the number of decision variables as is showed in UF5, UF10 and DTLZ4 with three objectives.
%
Also, the GDE3 show a non-stable performance with the parameter configurations, as is explained by J. Lampinen et al.\cite{Joel:GDE3_CEC09}, where they indicate that a high value for CR might lead to premature convergence with respect to one objective compared to another.

%
Specifically, the instance UF10 with GDE3 has an increment of HV for 100 variables, this irregularity is related with diversity issues \cite{Joel:GDE3_CEC09}
%
On the other hand the VSD-MOEA is enough stable, also it provides the best HV values.
%
It is interesting that the MOEA/D and MOMBI-II required an extra-parametrization, hence the stability could be compromised.

%
The 50\% attainment surfaces WFG2, WFG8 and DTLZ7 instances are showed in the figure \ref{fig:Attainment_Surfaces}.
%
The analyses shows that our proposal provide solutions approximated to the Pareto front.
%
Although the GDE3 approximate the WFG2 and DTLZ7, this this algorithm is far in some regions of the Pareto front with the WFG8 instance. 

\begin{figure}[t]
\centering
\input{Graphic-Scalability-2obj.tex}
\label{fig:variable-decision-scalability-2obj}
\end{figure}


\begin{figure}[t]
\centering
\input{Graphic-Scalability-3obj.tex}
\label{fig:variable-decision-scalability-3obj}
\end{figure}



\subsection{Analyzes of the Intial Factor Distance}

\begin{figure}[t]
\centering
\input{Graphic-Initial-Distance.tex}
\label{fig:Initial-distance-factor}
\end{figure}

\subsection{Convergence of the Diversity of WFG1}
\begin{figure}[t]
\centering
\input{Graphic-Diversity_WFG1.tex}
\label{fig:Diversity_WFG1}
\end{figure}


\begin{figure}[t]
\centering
\input{Graphic-Diversity_WFG5.tex}
\label{fig:Diversity_WFG5}
\end{figure}


\begin{figure}[t]
\centering
\input{Graphic-Diversity_WFG6.tex}
\label{fig:Diversity_WFG6}
\end{figure}


\subsection{Improving Dependece on the Execution}


\begin{figure}[t]
\centering
\input{Graphic-Performance-Time.tex}
\label{fig:Performance_time}
\end{figure}



\input{Tables/Table_HV_2obj.tex}
\input{Tables/Table_HV_3obj.tex}
%\input{Tables/Table_IGDP_2obj.tex}
%\input{Tables/Table_IGDP_3obj.tex}
\input{Tables/Tests_HV_2obj.tex}
\input{Tables/Tests_HV_3obj.tex}
%\input{Tables/Tests_IGDP_2obj.tex}
%\input{Tables/Tests_IGDP_3obj.tex}

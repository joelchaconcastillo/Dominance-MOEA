This section is devoted to fully describe our novel proposal.
%
There are two main contributions in this paper.
%
First, a novel objective-space density estimator is proposed.
%
Second, a novel replacement phase that takes into account the diversity in the variable space is devised.
%
The replacement phase does not only considers variable space diversity.
%
Instead it integrates information of both the variable and objective space.
%
Particularly, the novel objective-space density estimator is used inside the replacement phase.
%
The main principle behind the design of the novel replacement is to use the stopping criterion and 
elpased generations with the aim of gradually moving from exploration to exploitation during the search process.
%
Note that this principle might be incorporated in any of the three categories of \MOEAS{}.
%
In this paper, our decision was to incorporate it in a dominance-based approach and apply it to problems with 
a low number of objectives.
%
Particularly, our proposal is quite standard except for the replacement phase.
%
Algorithm~\ref{fig:vsd-moea} shows the pseudocode of \VSDMOEA{}.
%
The parent selection is performed with binary tournament based on the dominance raking with ties broken randomly.
%
The variation stage is based on applying the well-known Simulated Binary Crossover (SBX) 
and polynomial mutation~\cite{Joel:SBX1994, Joel:Mutation}.
%
The rest of this section are devoted to describe the replacement phase and the novel objective-space density 
estimator.

\begin{figure}[t]
\centering
\input{Diagram.tex}
%\includegraphics[width=0.45\textwidth]{Images/Fase_Remplazo_2.eps}
\caption{Penalty Method of the Replacement Phase - The left side represents the variables space and the right side the 
objective space}
\label{fig:Hypersphere}
\end{figure}


\subsection{Replacement Phase of VSD-MOEA}

The replacement phase of \EAS{} is in charge of deciding in each generation which are the survivors 
among the members of the previous population and offspring.
%
The novel replacement promotes a gradual movement from exploration to exploitation, which has been a quite i
beneficial principle in the design of single-objective optimizers~\cite{Joel:MULTI_DYNAMIC}.
%
Particularly, the replacement phase operates as follows.
%
First, the members of the previous population and offspring are joined in a multi-set with $2 \times N$ individuals.
%
Then, $N$ individuals must be selected to survive, which is performed with an iterative process that selects an additional
individual at each step.
%
In order to take into account the diversity in the decision space, the Distance to Closest Neighbor (\DCN{}) of each
individual is calculated at each step.
%
Thus, if the multi-set containing the currently selected survivors is called $S$, then the \DCN{} of an individual $I$ is calculated
as $\displaystyle{\min_{s \in S}\ Distance(I, s)}$.
%
Normalized Euclidean distances are considered, so in order to calculate distances between any two individual $A$ and $B$, 
Eq. (\ref{eqn:distance}) is applied.
%
\begin{equation}\label{eqn:distance}
Distance(A, B) =   \left ( \frac{1}{n}  \sum_{i=1}^n \left ( \frac{A_i - B_i}{x_i^{(U)} - x_i^{(L)}} \right )^2  \right)^{1/2}
\end{equation}

Note that individuals with large \DCN{} values contribute significantly to promote exploration.
%
In order to avoid an excessive decrease of the exploration degree, individuals with a \DCN{} value lower 
than a threshold value are penalized and they can only be selected if non-penalized individuals do not exist.
%
Then, among the non-penalized individuals, an objective-space density estimator is used to select the additional
survivor of the iteration.
%
In our case, the novel density estimator described in the next subsection is used. 

In order to better visualize the penalty method, it can be considered that after selecting each survivor, a hyper-sphere 
centered in such candidate solution --- in the variable space --- is created.
%
Then, all the individuals that are inside a hyper-sphere are penalized and the objective-space estimator takes 
into account only the non-penalized individuals.
%
This is illustrated in Fig.~\ref{fig:Hypersphere}, which represents a state where three individuals have been 
selected to survive and an additional survivor must be picked up.
%
The left side shows individuals in the variable space.
%
Current survivors are marked with a red border and each one of them is surrounded by a blue dash circle with 
radius $D_t$.
%
In this situation, the penalized individuals are the number 4, 5, and 6.
%
In the objective space ---right side --- penalized individuals are shown with gray background, indicating
that the objective-space density estimator can not select them.

Since penalizing with a large threshold value --- radius of the hyperspheres --- induces a higher degree of 
exploration, it makes sense to reduce this value during the optimization process.
%
This is precisely one of the keys of our proposal.
%
The sizes of the hyper-spheres are modified dynamically by taking into account the stopping 
criterion and elapsed generations.
%
Particularly, the radius is decreased in a linear way starting from an initial distance.
%
This means that in the initial phases exploration is promoted.
%
However, as the size of the radius decreases only very close individuals are penalized, meaning that more 
exploitation is performed.
%
Note that this method requires a parameter which is the initial radius of the hyper-spheres which is denoted as $D_I$. 
%
Setting this parameter with a large value might provoke the penalization of a lot of individuals, 
thus non-useful diversity might be maintained.
%
However, too small values might not prevent fast convergence and therefore the approach with such a parameterization 
might behave as a traditional non-diversity based approach.
%
The robustness of the proposal with respect to this additional parameter is studied in our experimental validation.

\begin{algorithm}[t]
\algsetup{linenosize=\tiny}
  \scriptsize
	\caption{Replacement Phase of VSD-MOEA} 
\begin{algorithmic}[1]
\STATE Input: $P_t$ (Population of current generation), $Q_t$ (Offspring of current Generation)
    	\STATE Output: $P_{t+1}$ 
        \STATE $R_t = P_t \cup Q_t$ \label{alg:1}
        \STATE $P_{t+1} = \emptyset$ \label{alg:2}
        \STATE $Penalized = \emptyset$ \label{alg:3}
				\STATE $D_t = D_I - D_I * \frac{G_{Elapsed}}{0.9*G_{End}}$ \label{alg:4}
				\FOR{$k \in {1...M}$}
					\STATE Move to $P_{t+1}$ the individual that optimize $AWF_k$ (Eq.~\ref{eqn:extremes})
				\ENDFOR
        \WHILE{ $|P_{t+1}|$ $\leq$ N } \label{alg:6}
					\STATE Compute $DCN$ of individuals in $R_t$ with $P_{t+1}$ used as reference set \label{alg:7}
					\STATE Move to $Penalized$ the individuals in $R_t$ with $DCN < D_t$  \label{alg:8}
        	\IF{$R_t$ is empty} \label{alg:9}
						\STATE Compute $DCN$ of individuals in $Penalized$ with $P_{t+1}$ used as reference set \label{alg:10}
						\STATE Move to $R_t$ the individual in $R_t$ with largest $DCN$ \label{alg:11}
        	\ENDIF
					\STATE $non-dominated-rank-assignment(R_t \cup P_{t+1}) $ \label{alg:12}
					\STATE Use the novel density estimator to select a new survivor from $R_t$ and move it to $P_{t+1}$\label{alg:13}
        \ENDWHILE
    	\RETURN $P_{t+1}$ \label{alg:14}
	\end{algorithmic}
\label{alg:Replacement_Phase}
\end{algorithm}


Algorithm~\ref{alg:Replacement_Phase} fully describes the replacement phase of \VSDMOEA{}.
%
First, the population of the previous generation ($P_t$) and the offspring ($Q_t$) are joined
in $R_t$ (line \ref{alg:1}).
%
The multiset $R_t$ contains, in each iteration, the remaining non-penalized individuals that might be selected 
to survive.
%
The population of survivors ($P_{t+1}$) and the set containing the penalized individuals are initialized to
the empty set (lines \ref{alg:2} and \ref{alg:3}).
%
Then, the threshold value ($D_t$) that is used to penalize too close individuals is calculated (line \ref{alg:4}).
%
Note that $D_I$ denotes the initial radius or threshold value, $G_{Elapsed}$ is the amount of generations that have 
been evolved, and $G_{End}$ is the stopping criterion, i.e. the number of generations that are to be evolved 
in the execution of \VSDMOEA{}.
%
The linear decrease is calculated so that after the $90\%$ of the generations, the $D_t$ value is lower than 0, 
meaning that no penalties are performed.
%
This means that in the first $90\%$ of the generations, more exploration than in traditional \MOEAS{} is induced, 
whereas in the final stages, a traditional \MOEA{} is applied.
%
Finally, for each objective a hiqh-quality candidate solution is selected to survive.
%
Note that selecting the best solution for each objective might provoke some drawbacks realated to accepting small improvement
in an objective at the cost of important worsening in other objectives~\cite{deb2016optimality}.
%
To solve this issue augmented functions can be applied, which has been the alternative used in this paper.
%
Particularly, for each objective $k$, the candidate solution that minimizes the Augmented Weighted Function (AWF)
given in Eq.~\ref{eqn:extremes} is selected and consequently moved to $P_{t+1}$ (line~\ref{alg:5}).
%
Note that, augmented functions usually take into account weight vectors with the aim of dealing with objectives
that present very different scales.
%
Since benchmarks that have similar scales in each objective have been used in this paper, there was no need to apply
such weight vectors.

\begin{equation}\label{eqn:extremes}
AWF_k (\vec{x}) = f_k(\vec{x}) + 10^{-4} \times  \sum_{j=1}^m f_j( \vec{x} )
\end{equation}


Then, an iterative process that selects an individual at each iteration is executed until the survivors
set contains $N$ individuals (line \ref{alg:6}).
%
The iterative process works as follows.
%
First, the \DCN{} value of each reamining non-penalized individual is calculated (line \ref{alg:7}).
%
Then, those individuals with a \DCN{} value lower than $D_t$ are moved to the set of penalized individuals (line \ref{alg:8}).
%
If all the remaining individuals are penalized (line \ref{alg:9}), it means that the amount of exploration is lower than expected.
%
Thus, the individual with largest \DCN{} values is recovered, i.e. moved to the non-penalized individuals 
set (lines \ref{alg:10} and \ref{alg:11}).
%
Finally, the objective space is taken into account.
%
Specifically, the candidate individuals and the current survivors are joined.
%
Then, the \textit{fast-non-dominated-sort} procedure is executed with such a set, stopping as soon as a front with 
a candidate individual is found (line \ref{alg:12}).
%
Then, for each candidate individual that belongs to the lowest front, the individual with higher contribution to 
the diversity in the objective space is selected (line \ref{alg:13}).
%
The specific way in which the diversity in the objective space is measured is described in the next section.
%

%Note also that, as part of the diversity calculation of the variable space, a metric should be selected.
%
%Since our experimental validation is performed with a continuous domain, the normalized Euclidean distance is used.
%%
%However, in discrete domains other distance metrics such as the Manhattan, or the Hamming distance might be considered, and the definition of
%such distance might affect the performance of the approach~\cite{Segura:17}.


%The main idea of replacement phase dwell in compute the contribution by each individual to diversity  in both spaces.
%
%Hence in each iteration just one individual is selected as survivor until the size of population is reached.
%
%Consequently this methodology separate the suitable candidates based in diversity of the variable space, afterward the best candidate in objective space is selected.
%

%
%The figure \ref{fig:Hypersphere} shows one iteration of the Replacement Phase where the reference individuals are $\{1, 2, 7\}$ and candidate individuals are $\{3, 4, 5, 6, 8\}$. 
%
%The individuals $\{4, 5, 6\}$ are moved to penalized set, since that each one is inside of a hypersphere (gray dotted circles) related to a reference individual.
%
%Finally, the candidate solutions are $\{2, 8\}$, due that both belongs to the same rank, the solution $8$ is selected since it has a better contribution to the diversity in the objective space.

%
%
%
%
% %The function definitions from algorithm~\ref{alg:Replacement_Phase} are explained as follow:
% %
% \begin{itemize}
% \item Diversity\_Variable\_Space(A,B): for each individual from set A an euclidean distance is computed in variable space to the closest solution from the set B.
% \item Diversity\_Objective\_Space(A,B): for each individual from set A an euclidean distance is computed in objective space to the closest solution from the set B.
%\end{itemize}
%
%
%

%ESto hay que moverlo a Experimental Validation
%Due that the initial diversity is influenced by  $D_I$ parameter, several experiments with different values have been realized.
%
%Accordingly that each dimension is normalized in the unity, the maximal hypersphere of the variable decisions is denoted by $\sqrt{N}$ where $N$ is the dimension of the variable space. 
%
%Under those circumstances, the hypersphere with radius $\sqrt{N}$ will cover outside from the bounds, for this reason it is multiplied by a factor $k$  resulting in the formula$D_I = k * \sqrt[]{N}  \rightarrow k \in (0,1)$.
%
%Empirically, the ideal configuration that provides quality solutions is $k=0.25$ .
%
%Additionally, the algorithm empirically is enough stable, given that different values of $k$ does not provide drastically differences in the solutions.

\subsection{A Novel Density Estimator for the Objective Space}
\label{subsection:density}

Since the dominance definition is not related to the preservation of diversity in the objective space,
dominance-based \MOEAS{} incorporate special procedures to maintain diverse solutions, such as clustering and/or crowding.
%
In this paper, we define a novel distance metric, and an iterative heuristic selection approach, which selects an individual of the best front and with the largest defined distance.
%
Specifically, the novel distance is called ``Improvement Distance'' (ID) and it 
follows the same principles that guided the design of both indicators the IGD+ and $I_{\epsilon +}$~\cite{Joel:Inverted_Generational_Distance_Plus}, \cite{Joel:IBEA}, \cite{zitzler2003performance}.
%
The main idea is to prefer those individuals whose quality in all objectives is similarly preserved.
%
Particularly, a non-dominated individual can be very distant to the Pareto Front due that such individual could be the best in one objective but mainly deteriorated in the rest of objectives, so as result it has high diversity in objective space.
%----
%The key idea is that, a direct Euclidean distance is used, those individuals that have very high values in some
%of the objectives might present very large distances which is not an adequate property.
%
In fact, high improvements in one objective value are related to larger selection probabilities and not the opposite, so this behavior should be avoided.
%
%In order to take this principle into account, when calculating the distance between two individuals

The key idea takes into account the dominance relation between the candidate and reference individuals. % when the distance is computed.
%
%
Consequently, the reference and the candidate individuals are compared.
%
If the reference individual is dominated by the candidate individual, then the euclidean distance with no modification is implemented.
%
However if they are non-dominated with each other, then is calculated the minimum distance from the reference individual to the dominated region by the candidate individual. %
%
Additionally, if the candidate individual is dominated by the reference individual, then is computed the $I_\epsilon$ indicator, that gives the minimum distance by which the candidate individual needs to or can be translated in each dimension in objective space such that the reference individual is dominated.
%
Therefore, this distance can be viewed as an amount of inferiority of the solution in comparison with the reference individual.	
%
The improvement distance is defined in Equation (\ref{eq:ImprovementDistance}) which incorporates the $I_\epsilon$ indicator (Equation \ref{eqn:epsilonindicator})  where $R$ and $C$ are the reference and candidate solutions respectively. 
\begin{equation} \label{eq:ImprovementDistance}
\begin{split}
 ID(R, C) = &  \left (\sum_{i=1}^M \left (max(0, R_i - C_i \right ))^2  \right)^{1/2} - I_\epsilon(R,C) \\
% &I_\epsilon(R,C) = min_{\epsilon} \{ f_i(C) - \epsilon \leq f_i (R) \quad \forall i \in \{1,..,m \} \}
\end{split}
\end{equation}
\begin{equation}\label{eqn:epsilonindicator}
\begin{split}
I_\epsilon(R,C) = \begin{dcases}
   min_{\epsilon} \{ f_i(C) - \epsilon \leq f_i (R) \} \} & R \preceq C \\
%\quad \forall i \in \{1,..,m \} \\
    0 & otherwise
	\end{dcases}
\end{split}
\end{equation}

Specifically, this distance is considered as a Weakly Pareto Compliant Indicator.
%
In addition, this metric relaxes some difficulties encountered when the number of objectives is increased, given that the solutions in many objectives are usually non-dominated with each other by using the Pareto dominance relation.
%
This means a very low selection pressure toward the Pareto front in Pareto-dominance-based MOEAS \cite{Joel:Optimization_Of_Scalarizing_Functions_Through_Evolutionary_MOEAS}.
%
Principally, the improvement distance is effective with over-prioritization  of dominance-resist solutions i.e., solutions with exceptional performance in one objective and extremely poor performance in many others \cite{Joel:Failure_MOEAs}.
%

Given that the design of this MOEA is considered for long-term executions, the algorithm \ref{alg:Replacement_Phase} is implemented efficiently.
%
Thus, the distances are pre-computed and are updated in each iteration, the same for the dominance count information that is used in the \textit{conditionally-non-dominated-sort}.
%
In fact, the worst case complexity of this algorithm is $O((3 N)^2 \times n)$, since that the dimension of the decision variable space is usually bigger than the number of objectives. 

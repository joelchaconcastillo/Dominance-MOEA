This section describes the experimental validation carried out to study the performance and
gain a clear understanding of the specifics of our proposed \VSDMOEA{}.
%
Our results clearly show that controlling the diversity of decision variable space provides a way to further improve the results 
obtained by state-of-art \MOEAS{}.
%
First, we discuss some technical specifications involving the benchmark problems and algorithms implemented.
%
We then present a comparison between \VSDMOEA{} and state-of-the-art algorithms in long-term executions.
%
Then, three additional experiments to fully validate \VSDMOEA{} are included.
%
These analyses are designed to test the scalability in decision variable space, the performance with different stopping criteria, 
and the behavior with different initial penalty thresholds.

This work takes into account some of the most popular and widely used benchmarks in the multi-objective field.
%
These problems are the WFG~\citep{Joel:WFG}, DTLZ~\citep{Joel:DTLZ}, and UF~\citep{Joel:CEC2009} test suites 
configured in a standard way.
%
The WFG test problems were used with two and three objectives and 
were configured with $24$ parameters, $20$ of them corresponding to distance parameters and $4$ to position parameters.
%
In the DTLZ test problems, the number of variables was set to $n=M+r-1$, where $r=\{5, 10, 20\}$ for DTLZ1, DTLZ2 to DTLZ6 and DTLZ7, respectively.
% 
The UF benchmark comprises seven problems with two objectives (UF1-7) and three problems with three objectives (UF8-10).
%
All of them were configured with $30$ variables.
%
Note that the experiment used to analyze scalability considers different numbers of variables.

The experimental validation includes three well-known state-of-the-art \MOEAS{} and \VSDMOEA{}.
%
The \MOEAS{} that are considered are \NSGAII{}~\footnote{\url{http://jmetalcpp.sourceforge.net/}}, \MOEAD{}~\footnote{\url{http://www3.ntu.edu.sg/home/epnsugan/index_files/CEC09-MOEA/CEC09-MOEA.htm}}, and \RMOEA{}~\footnote{\url{http://inriadortmund.gforge.inria.fr/r2emoa/}}, 
which can be classified as dominance-based, decomposition-based, and indicator-based, respectively.
%
Note that for the indicator-based category, the s-metric selection evolutionary multiobjective optimisation algorithm 
(\textsc{sms-emoa})~\citep{Joel:SMSEMOA} was also taken into account initially.
%
However, due to its high computational cost, it is not convenient for long-term experiments as the ones used in this paper.
%
For instance, a single execution of \textsc{sms-emoa} took more than 15 days to complete, meaning that executing several
repetitions with several functions is not feasible.
%
In the case of \MOEAD{}, several variants have been devised.
%
The \MOEAD{} implementation considered is the one that obtained first place in the IEEE 2009 Congress on Evolutionary Computation's 
MOP Competition proposed by~\cite{zhang2009performance}.
%
The common configuration in all the experiments was as follows: the population size was set to $100$, and the genetic 
operators were Simulated Binary Crossover (SBX) and polynomial-based 
mutation~\citep{Joel:SBX1994, Joel:Mutation}.
%
The crossover probability was set to $0.9$ and the crossover distribution index was set to $2$.
%
Similarly, the mutation probability and distribution index were fixed to $1/n$ and $50$, respectively.
%
The additional parameterization required by each algorithm is shown in Table~\ref{tab:Parametrization}.
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[t]
\centering
\caption{ Parameterization applied to each MOEA}
\label{tab:Parametrization}
\begin{tabular}{c|c}
\hline
\textbf{Algorithm} & \textbf{Configuration} \\ \hline
\multirow{3}{*}{\textbf{MOEA/D}} &Max. updates by sub-problem ($\eta_r$) = 2, \\
 & tour selection = 10,   neighbor size = 10, \\
 & period utility updating = 30 generations, \\ 
 & local selection probability ($\delta$) = 0.9,\\ \hline
\textbf{VSD-MOEA} & $D_I=0.4$ \\ \hline
\textbf{R2-EMOA} & $\rho=1$, offspring by iteration = $1$ \\ \hline
\end{tabular}
\end{table}

%
Note that scalarization functions are required in \MOEAD{} and \RMOEA{}.
%
In both cases, the Tchebycheff approach is used.
%
The procedure for generating the weight vectors differs in \MOEAD{} and \RMOEA{}.
%
\RMOEA{} was applied with $501$ and $496$ weight vectors for two and three objectives, respectively~\citep{trautmann2013r2}.
%
In contrast, \MOEAD{} requires the same number of weight vectors as the population size.
%
They were generated using the uniform design (UD) and the good lattice point (GLP) method~\citep{Joel:MOEAD_Uniform_Design, Joel:Kuhn_Munkres}.

Given that all the algorithms considered are stochastic, each execution was repeated $35$ times with different seeds.
%
The hypervolume indicator (\HV{}) is used to compare results.
%
Note that in the supplementary material, the results are also compared in terms of the IGD+ metric, with the conclusions being quite similar.
%
The reference point used to calculate the \HV{} is chosen to be a vector whose values are sightly larger (ten percent) than the nadir point, 
as suggested in~\cite{ishibuchi2017reference}.
%
The normalized \HV{} is used to facilitate the interpretation of the results~\citep{li2015evolutionary},
and the value reported is computed as the ratio between the normalized \HV{} obtained and the maximum attainable 
normalized \HV{}.
%
In this way, a value equal to one means a perfect approximation.
%
Note that a value equal to one is not attainable because \MOEAS{} yield a discrete approximation.
%
Finally, in order to statistically compare the \HV{} ratios, a guideline similar to that proposed in~\cite{Joel:StatisticalTest} was used. 
%
First a Shapiro-Wilk test was performed to check if the values of the results followed a Gaussian distribution. 
%
If so, the Levene test was used to check for the homogeneity of the variances. 
%
If the samples had equal variance, an ANOVA test was done; if not, a Welch test was performed. 
%
For non-Gaussian distributions, the non-parametric Kruskal-Wallis test was used to test whether samples are drawn from the same distribution. 
%
An algorithm $X$ is said to beat algorithm $Y$ when the differences between them are statistically significant, and the mean and median \HV{} ratios 
obtained by $X$ are higher than the mean and median achieved by $Y$.

%

\subsection{Comparison against State-of-the-art \MOEAS{} in long-term executions}

Our first experiment aims to compare the long-term performance of \VSDMOEA{} against state-of-the-art proposals, which
is the kind of execution where diversity-based \EAS{} have been more successful.
%
Specifically, the stopping criterion was set to $2.5 \times 10^7$ function evaluations.
%

%
\input{Tables/Table_HV_2obj.tex}
Table~\ref{tab:StatisticsHV_2obj} shows the \HV{} ratio obtained for the benchmark functions
with two objectives.
%
Specifically, the minimum, maximum, mean and standard deviation of the \HV{} ratio is shown for each method and problem tested.
%
The last row shows the results considering all the test problems together.
%
For each test problem, the data for the method that yielded the largest mean is shown in {\bf boldface}.
%
Additionally, all the methods that were not statistically inferior than the method with the largest mean 
are shown in {\bf boldface}.
%
From here on, the methods shown in {\bf boldface} for a given problem are referred to as the winning methods.
%
Based on the number of test problems where each method is in the group of the winning methods for the cases 
with two objectives, the best methods are \VSDMOEA{} and \RMOEA{} with 12 and 8 wins, respectively.
%
Thus, \VSDMOEA{} is the most competitive method in terms of this metric.
%
More impressive is the fact that the mean \HV{} ratio attained by \VSDMOEA{}, when all the problems are considered simultaneously, is much higher
than the one attained by \RMOEA{}.
%
In fact, the total means of \RMOEA{} ($0.882$), \NSGAII{} ($0.886$) and \MOEAD{} ($0.881$) are quite similar.
%
In contrast, \VSDMOEA{} achieved a much higher value ($0.949$).
%
Inspecting the data carefully, it is clear that in the cases where \VSDMOEA{} loses, the difference with respect to the
best method is not very large.
%
For instance, the difference between the mean \HV{} ratio attained by the best method and by \VSDMOEA{} was never larger
than $0.1$.
%
However, all the other methods exhibited a deterioration greater than $0.1$ in several cases.
%
Specifically, it happened in $5$, $5$ and $6$ problems for \RMOEA{}, \NSGAII{} and \MOEAD{}, respectively.
%
This means that even if \VSDMOEA{} loses in some cases, its deterioration is always small, exhibiting a much more 
robust behavior than any other method.

\input{Tables/Tests_HV_2obj.tex}

In order to better clarify these findings, pair-wise statistical tests were done among each method tested in each
test problem.
%
For the two-objective cases, Table~\ref{tab:Tests_HV_2obj} shows the number of times that each method won (column $\uparrow$),
lost (column $\downarrow$), tied (column $\leftrightarrow$) and \textbf{Score}.
%
The later is calculated as the diference between the number of times that each method won and the numer of times that each method lost.
%
Additionally, for each method $M$, we calculated the sum of the differences between the mean \HV{} ratio attained by the best method 
(the ones with the highest mean) and method $M$, for each problem where $M$ was not in the group of winning methods.
%
This value is shown in the Deterioration column.
%
The data confirms that although \VSDMOEA{} loses in some cases, the overall numbers of wins and losses
favors \VSDMOEA{}.
%
More importantly, the total deterioration is quite lower in the case of \VSDMOEA{}, confirming that when \VSDMOEA{} loses, the deterioration is not 
that large.

\input{Tables/Table_HV_3obj.tex}
\input{Tables/Tests_HV_3obj.tex}

Tables~\ref{tab:StatisticsHV_3obj} and~\ref{tab:Tests_HV_3obj} show the same information for the problems with three objectives.
%
In this case, the superiority of \VSDMOEA{} is even clearer.
%
Taking into account the mean of all the test problems, \VSDMOEA{} again obtained a much larger mean \HV{} ratio than the other methods.
%
Specifically, \VSDMOEA{} obtained a value of $0.918$, whereas the second ranked algorithm (\RMOEA{}) obtained a value of $0.855$.
%
Once again, the difference between the mean \HV{} ratio obtained by the best method and by \VSDMOEA{} was never greater
than $0.1$.
%
However, all the other methods exhibited a deterioration greater than $0.1$ in several cases.
%
In particular, this happened in $5$, $6$ and $6$ problems for \RMOEA{}, \NSGAII{} and \MOEAD{}, respectively.
%
Moreover, in this case, \VSDMOEA{} is much superior than the other methods not only in terms of total deterioration, but also
in terms of total wins and losses (see Table~\ref{tab:Tests_HV_3obj} and data shown in {\bf boldface} 
in Table~\ref{tab:StatisticsHV_3obj}).
%
\VSDMOEA{} was in the group of winning methods for 16 out of 19 test problems, whereas the second best-ranked algorithm (\RMOEA{})
was in the group of winning methods for only 3 test problems.

\subsection{Decision Variable Scalability Analysis}

In order to study the scalability of \VSDMOEA{} in terms of the number of decision variables, all of the algorithms already described were tested with
the same benchmark problems, but considering $50$, $100$, and $250$ variables.
%
Note that in the WFG test problems, the number of position parameters ($k$) and distance parameters ($l$) must be specified.
%
Specifically, the number of distance parameters was set to $42$, $84$, and $210$ when using $50$, $100$ and $250$ variables, respectively.
%
The rest of the decision variables were position parameters, meaning that they were $8$, $16$ and $40$, respectively.
%
Note that increasing the number of variables greatly increases the computing time required.
%
As a result, this study takes into account middle-term executions.
%
%Specifically, the stopping criterion was set to $25,000$ generations.
Specifically, the stopping criterion was set to $2.5 \times 10^6$ function evaluations.
%
Figure~\ref{fig:variable-decision-scalability} shows the mean \HV{} ratio for the four algorithms tested,
considering the problems with two and three objectives.
%
As expected, the \HV{} ratio decreases as the number of variables increases.
%
In the two-objective case, the deterioration is similar in every algorithm, so the superiority of \VSDMOEA{} is clear regardless of the number of 
variables.
%
In contrast, in the three-objective case, the deterioration of \VSDMOEA{} is higher than for \RMOEA{} and \MOEAD{}.
%
In fact, when considering $250$ variables, the performance of \VSDMOEA{} is just slightly superior to that of \RMOEA{}.

\begin{figure}[h]
\centering
\begin{tabular}{c c}
\includegraphics[scale=0.72]{Images/Graphic-Scalability-2obj_tikz-figure0.eps} & \includegraphics[scale=0.72]{Images/Graphic-Scalability-3obj_tikz-figure0.eps}
\end{tabular}
\caption{Mean of the \HV{} ratio for 35 runs for the two-objective (left side) and three-objective (right side) problems considering different numbers of variables}\label{fig:variable-decision-scalability}
\end{figure}

%\begin{figure}[t]
%\centering
%\includegraphics[scale=0.85]{Images/Graphic-Scalability-3obj_tikz-figure0.eps}
%\caption{Mean of the \HV{} ratio for 35 runs for the three-objective problems considering different numbers of variables} \label{fig:variable-decision-scalability-3obj}
%\end{figure}

In order to better understand this behavior, we selected problems WFG1 to WFG7.
%
The WFG test problems divide the variables into two kinds of parameters (this framework uses the term parameter instead of 
variable): the distance parameters and the position parameters.
%
Note that a parameter $i$ is a distance parameter when for all $\vec{\mathbf{x}}$, modifying $x_i$ results in a new solution 
that dominates $\vec{\mathbf{x}}$, is equivalent to $\vec{\mathbf{x}}$, or is dominated by $\vec{\mathbf{x}}$.
%
However, if $i$ is a position parameter, modifying $x_i$ in $\vec{\mathbf{x}}$ always results in a vector that is incomparable or 
equivalent to $\vec{\mathbf{x}}$~\citep{huband2005scalable}.
%
Additionally, note that we selected problems WFG1-WFG7 because their distance parameter values associated to all Pareto optimal solutions 
have exactly the same values:
%
\begin{equation}
   x_{i=k+1:n} = 2i \times 0.35
\end{equation}
%
This is very important because it has been shown that for these cases, state-of-the-art
\MOEAS{} might provoke a quick convergence in \textit{distance parameters}, resulting in an effect that is similar to 
premature convergence in the single-objective case~\citep{Joel:GDE3_CEC09,castillo2017multi}.

\begin{figure}[t]
\centering
\setlength{\tabcolsep}{0.0em}
\begin{tabular}{c c}
\includegraphics[scale=0.75]{Images/Graphic-Diversity_2obj_tikz-figure1.eps} & \includegraphics[scale=0.75]{Images/Graphic-Diversity_3obj_tikz-figure1.eps}
\end{tabular}
\caption{Evolution of ADI for problems WFG1-WFG7 with two and three objectives considering only the distance variables}\label{fig:Diversity}
\end{figure}

For each algorithm, we calculated the average (mean) Euclidean distance among individuals (ADI) in the population by considering only 
the distance parameters.
%
Figure~\ref{fig:Diversity} shows how the ADI evolves for the two-objective (left side) and three-objective (right side) problems.
%Figures~\ref{fig:Diversity_2obj} and~\ref{fig:Diversity_3obj} show how the ADI evolves for the two-objective and three-objective problems.
%
The behavior of \NSGAII{} and \MOEAD{} --- which are not included --- is similar to that of \RMOEA{} in terms of how the ADI evolves. 
%
Thus, to avoid saturating these figures, only the information for \VSDMOEA{} and \RMOEA{} with 50, 100 and 250 variables is shown.
%
The first obvious fact is that \VSDMOEA{} converges much slower than \RMOEA{}.
%
Accordingly, the difference between the diversity maintained in the first generation and that maintained after 10\% of the execution,
is much larger in \RMOEA{} than in \VSDMOEA{}.
%
In the case of \VSDMOEA{}, the decrease in ADI is quite linear until the halfway point of the execution.
%
This is due to the way in which the threshold distance value ($D_t$) is calculated.
%
Additionally, a closer inspection of the data reveals other important aspects that must be discussed. 
%
In the two-objective case, increasing the number of variables causes the diversity in the \RMOEA{} to increase slightly.
%
However, the amount of diversity is low even when using 250 variables, meaning that incorporating mechanisms to increase diversity --- as is done in \VSDMOEA{} ---
is very helpful.
%
In contrast, in the three-objective case, the amount of diversity in \RMOEA{} is not as low.
%
Moreover, increasing the number of variables yields a significant increase in the resulting ADI, meaning that in this case,
fast convergence is not an important issue.
%
These results show that, as the number of objectives and variables increases, \MOEAS{} tend to maintain a higher variable space diversity
in an implicit way, meaning that explicitly controlling the variable space diversity is probably not as important.
%

Finally, it is worth noting that we selected some problems to conduct long-term executions with 250 variables.
%
\VSDMOEA{} was able to further improve the results when using long-term executions, while the other state-of-the-art algorithms did not yield significant improvements.
%
This probably means that as technology evolves, allowing longer executions to be carried out in reasonable time frames,
the incorporation of explicit control of diversity will be even more important.
%
Note that this also happens in the single-objective case, where the benefits of explicitly controlling diversity appears only when using executions lasting
several weeks when dealing with large instances of the Traveling Salesman Problem~\citep{segura2015novel}.
%

%\begin{figure}[t]
%\centering
%\includegraphics[scale=0.85]{Images/Graphic-Diversity_3obj_tikz-figure1.eps}
%\caption{Evolution of ADI for problems WFG1-WFG7 with three objectives considering only the distance variables}\label{fig:Diversity_3obj}
%\end{figure}

\subsection{Analysis of the Stopping criterion}

This section ilustrates the main reason behind the superiority of the \VSDMOEA{} against state-of-the-art algorithms in long-term executions.
%
As previously discussed, \EAS{} with explicit control of diversity often are more useful in long-term executions.
%
The fact that we selected a rather large stopping criterion in our first experiment might lead readers to think that \VSDMOEA{} is only
useful in extremely long-term executions;
%
however, this is not the case.
%
In this section we analyze the performance of \VSDMOEA{} and state-of-the-art algorithms with several stopping criteria, 
i.e., maximum number of function evaluations.
%
Three different ranges were explored for the stopping criterion.
%
Each range was split into ten equally distributed intervals, and experiments were run with each different number of function evaluations.
%
The ranges considered were $[2.5 \times 10^4, 2.5 \times 10^5]$, $[2.5 \times 10^5, 2.5 \times 10^6]$ and $[2.5 \times 10^6, 2.5 \times 10^7]$.
%
These ranges are referred to as short-term, middle-term and long-term executions, respectively.
%
Note that state-of-the-art algorithms can be executed just once (with $2.5 \times 10^7$ function evaluations) by saving the intermediate results.
%
However, \VSDMOEA{} makes decisions that depend on the stopping criteria, so independent executions were required for each stopping criterion.

\begin{figure}[t]
\centering
\setlength{\tabcolsep}{0.0em}
\begin{tabular}{cc}
\begin{tabular}{l}
 \includegraphics[scale=0.5]{Images/Time_tikz-figure0.eps}\\[0cm]%[-0.14cm] 
 \includegraphics[scale=0.5]{Images/Time_tikz-figure1.eps}\\[0cm]%[-0.18cm]
 \includegraphics[scale=0.5]{Images/Time_tikz-figure2.eps}
\end{tabular}
& \begin{tabular}{l}
 \includegraphics[scale=0.5]{Images/Time_tikz-figure3.eps}\\[0cm]%[-0.14cm] 
 \includegraphics[scale=0.5]{Images/Time_tikz-figure4.eps}\\[0cm]%[-0.18cm]
 \includegraphics[scale=0.5]{Images/Time_tikz-figure5.eps}
\end{tabular}

\end{tabular}
\caption{Performance of \MOEAS{} for the problems with two objectives (left side) and three objectives (right side) considering three ranges for the stopping criterion: 
short-term (first row), middle-term (second row) and long-term (third row).}\label{fig:Performance_time}
\end{figure}

Figure~\ref{fig:Performance_time} shows the mean \HV{} ratio obtained with each \MOEA{} 
%Figures~\ref{fig:Performance_time_2obj} and~\ref{fig:Performance_time_3obj} show the mean \HV{} ratio obtained with each \MOEA{} 
with two and three objectives, respectively.
%
All the problems were considered to calculate this mean ratio.
%
Each figure is divided into three graphs corresponding to short-term, middle-term and long-term.
%
In the two-objective case, for the shortest executions, \VSDMOEA{} is not very competitive.
%
In the range $[2.5 \times 10^4, 7.5 \times 10^4]$, it exhibits the worst performance, meaning that for very short-term executions,
explicitly promoting additional diversity is not helpful.
%
When using $10^5$ function evaluations, the resulting \HV{} ratio is similar than that obtained by other methods.
%
Finally, when using more than $2.5 \times 10^5$ function evaluations, the \HV{} ratio obtained by \VSDMOEA{} is much higher than the one
obtained by other methods.
%
It is worth noting that \VSDMOEA{} is the only method that truly takes advantage of using long-term executions,
with the remaining methods just showing a slight improvement.
%
In the three-objective case, \VSDMOEA{} yields a lower \HV{} ratio than \RMOEA{} and \MOEAD{} in short-term executions,
but as more function evaluations are executed, the differences decrease.
%
In this case, after $5 \times 10^5$ function evaluations, the performance of \VSDMOEA{} is similar to that of \RMOEA{}.
%
Finally, as in the two-objective case, with additional function evaluations, the differences between \VSDMOEA{} and the remaining
algorithms increase in favor of \VSDMOEA{}.
%
Thus, while the most important benefits arise in long-term executions, users can benefit from \VSDMOEA{} even in shorter executions.


\subsection{Analysis of the Initial Threshold Value}

One of the disadvantages of including a strategy for controlling diversity is that this is usually done at the expense of
incorporating additional parameters in the \EA{} designed.
%
In the case of \VSDMOEA{}, the initial threshold value ($D_I$) must be set.
%
The higher this value is, the greater the exploration of the decision variable space.
%
Note that in all the previous experiments, $D_I = 0.4$ was used.
%
This value was selected based on some preliminary experiments.
%
This section is devoted to analyzing the performance of \VSDMOEA{} when using different $D_I$ values. 
%
Note that, since normalized distances are used, the maximum difference that can appear is $1$.
%
Additionally, note that when $D_I$ is set to 0, no individual is penalized on the basis of its decision
variable space diversity contribution,
so \VSDMOEA{} would behave like a more traditional \MOEA{}.
%
As a result, the values $D_I = \{0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\}$ were tested.
%
As in previous experiments, the whole set of benchmark problems was used and
the stopping criterion was set to $2.5 \times 10^7$ function evaluations.

\begin{figure}[t]
\centering
\includegraphics[scale=0.85]{Images/Graphic-Initial-Distance_tikz-figure0.eps} \\
\caption{Mean of \HV{} values taking into account all the problems with several initial threshold values}\label{fig:Initial-distance-factor}
\end{figure}

Figure~\ref{fig:Initial-distance-factor} shows the mean \HV{} ratio obtained for both the two-objective 
and the three-objective case.
%
Note that even when $D_I$ is set to $0$, \VSDMOEA{} yielded better \HV{} ratios than other 
state-of-the-art algorithms (see Tables~\ref{tab:StatisticsHV_2obj} and~\ref{tab:StatisticsHV_3obj}).
%
Specifically, the values were $0.912$ and $0.893$ for two and three objectives, respectively.
%
This means that the novel density estimator put forth in this paper is indeed helpful.
%
However, the increase in performance when using other $D_I$ values is clear.
%
The \HV{} ratio obtained quickly increases as higher $D_I$ values up to $0.4$ are used.
%
Then, with values in the range $[0.5, 0.9]$, the performance decreases slightly.
%
There is a large range of values where the performance is very good, meaning that 
the behavior of \VSDMOEA{} is quite robust.
%
Thus, properly setting this parameter is not a complex task.
%
%\begin{figure}[t]
%\centering
%\begin{tabular}{l}
% \includegraphics[scale=0.6]{Images/Time_tikz-figure3.eps}\\[0cm]%[-0.14cm] 
% \includegraphics[scale=0.6]{Images/Time_tikz-figure4.eps}\\[0cm]%[-0.18cm]
% \includegraphics[scale=0.6]{Images/Time_tikz-figure5.eps}
%\end{tabular}
%\caption{Performance of \MOEAS{} for the problems with three objectives considering three ranges for the stopping 
%criterion: short-term (first row), middle-term (second row) and long-term (third row).}\label{fig:Performance_time_3obj}
%\end{figure}


